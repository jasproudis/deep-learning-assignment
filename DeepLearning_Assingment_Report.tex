\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{color}

\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\title{\textbf{Deep Learning Assignment Report -- 2024-25}}
\author{Iason-Christoforos Asproudis\\Student ID: p3352318}
\date{}

\begin{document}

\maketitle

\section*{Part 1: Image Classification}

\subsection*{Dataset Description}
We utilized two datasets for initial image classification experiments:
\begin{itemize}
    \item \textbf{Fashion-MNIST}: 28x28 grayscale images categorized into 10 clothing item classes.
    \item \textbf{CIFAR-10}: 32x32 RGB images across 10 object classes such as airplanes, cats, and cars.
\end{itemize}
Both datasets are provided through \texttt{tensorflow.keras.datasets}.

\subsection*{Model Architectures}

\textbf{Fashion-MNIST (MLP):}
\begin{itemize}
    \item Fully connected neural network using Functional API
    \item Two hidden layers with 256 and 128 units
    \item ReLU activation and Dropout (0.3)
    \item Glorot uniform initialization
\end{itemize}

\textbf{CIFAR-10 (CNN):}
\begin{itemize}
    \item Three convolutional blocks with Conv2D, BatchNormalization, MaxPooling, and Dropout
    \item Flatten and dense layers at the end
    \item Trained with EarlyStopping
\end{itemize}

\subsection*{Training Strategy}
\begin{itemize}
    \item Optimizer: Adam
    \item EarlyStopping with patience of 3
    \item Training epochs: 10--30
    \item Batch size: 128
    \item Dropout and BatchNormalization to reduce overfitting and stabilize training
\end{itemize}

\subsection*{Challenges \& Solutions}
\begin{itemize}
    \item Slow convergence on CIFAR-10: Deepened architecture and added normalization layers
    \item Overfitting in Fashion-MNIST: Resolved with EarlyStopping and Dropout
\end{itemize}

\subsection*{Results Summary}
\textbf{Fashion-MNIST (MLP):}
\begin{itemize}
    \item Test Accuracy: 88.7\%
    \item Validation Accuracy: 89.0\%
\end{itemize}

\textbf{CIFAR-10 (CNN):}
\begin{itemize}
    \item Test Accuracy: \textasciitilde71\%
\end{itemize}

\section*{Part 2: X-ray Classification -- MURA Dataset}

\subsection*{Dataset Overview}
The MURA dataset includes musculoskeletal radiographs categorized as either \textit{normal} or \textit{abnormal} across seven body parts. An analysis of the training split revealed:
\begin{itemize}
    \item Total labeled studies: 13,456
    \item Normal (label 0): 8280 (61.5\%)
    \item Abnormal (label 1): 5176 (38.5\%)
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Mura_Stats.png}
    \caption{heatmap of MURA Dataset}
    \label{fig:enter-label}
\end{figure}

Body parts are unevenly represented, with \textbf{wrist} and \textbf{shoulder} dominating the dataset. Notably, \textbf{shoulder} is the only body part where abnormal cases outnumber normal ones. This imbalance was visualized using a heatmap (Figure 1) and informed our training strategy.

\subsection*{Learning Approach}
The pipeline was designed to support:
\begin{itemize}
    \item Binary classification: Normal vs. Abnormal
    \item Multitask learning: Body part prediction as auxiliary task
    \item Modular design with reusable code components
    \item Resource optimization for Colab Pro (A100 GPU)
    \item Generalization using data augmentation and sample weighting
\end{itemize}

\subsection*{Model Development}

\textbf{CNN from Scratch:}
\begin{itemize}
    \item Custom multitask CNN with two output heads
    \item Batch normalization and dropout
    \item Metrics: accuracy, F1-score, precision, recall, categorical accuracy
\end{itemize}

\textbf{EfficientNetB0 (Transfer Learning) -- Failure:}
\begin{itemize}
    \item Despite ImageNet pretraining, validation metrics remained at 0.00
    \item Severe overfitting: training accuracy >85\% while val\_accuracy $\approx$ 4.3\%
    \item Switching to global average pooling and sigmoid did not help
    \item Transfer failed due to domain mismatch between natural and medical images
\end{itemize}

\textbf{Switch to InceptionV3 + MaxPooling:}
\begin{itemize}
    \item Adopted InceptionV3 with max pooling and regularized custom heads
    \item Addressed memory and shape issues with simplified multitask setup
    \item Augmentation, sample weighting, and fine-tuning reused
    \item Validation metrics started recovering as training stabilized
\end{itemize}

\subsection*{Data Handling}
\begin{itemize}
    \item Grayscale to RGB conversion
    \item Local caching and float16 usage for memory efficiency
    \item Data augmentation: flip, brightness, contrast
    \item Sample weights using \texttt{compute\_sample\_weight}
\end{itemize}

\subsection*{Results}

\textbf{CNN from Scratch (Multitask):}
\begin{itemize}
    \item Binary Accuracy: 64.9\%
    \item F1 Score (Abnormal): 0.60
    \item Body Part Accuracy: $\sim$100\%
    \item Precision (Abnormal): 0.66
    \item Recall (Abnormal): 0.56
\end{itemize}

\vspace{1em}
\noindent\textbf{Note:} InceptionV3 training still in progress. Metrics will be updated after full convergence.

\begin{figure}
    \centering
    \includegraphics[width=0.3\linewidth]{main_hybrid_inceptionV3_schema.png}
    \caption{Pretrained Model Architecture}
    \label{fig:enter-label}
\end{figure}

\subsection*{Challenges and Strategy Shifts}

\textbf{Challenges Encountered:}
\begin{itemize}
    \item GPU memory exceeded with multitask models
    \item Output shape mismatch in pretrained multitask architectures
    \item Crashes in Colab from RAM overflow
    \item Grayscale inputs incompatible with EfficientNetB0
    \item Class weight instability in multitask setup
    \item TensorFlow/Keras version incompatibilities
    \item Functional API input mismatches
    \item Overfitting with frozen base models
    \item Path inconsistencies between local and Colab
\end{itemize}

\textbf{Solutions Implemented:}
\begin{itemize}
    \item Switched to binary-only for pretrained models
    \item Used float16 and smaller batch sizes
    \item Replaced \texttt{ImageDataGenerator} with \texttt{tf.data.Dataset}
    \item Applied \texttt{sample\_weight} over \texttt{class\_weight}
    \item Data augmentation for robustness
    \item EarlyStopping and ReduceLROnPlateau
    \item Saved best checkpoints with timestamps
    \item Guided data decisions using heatmaps
\end{itemize}

\section*{Source Code}
All source code, including preprocessing, training, and model definition scripts, is available at: \\
\url{https://github.com/jasproudis/deep-learning-assignment}

\section*{Final Thoughts}
This assignment offered valuable hands-on experience across various deep learning tasks, ranging from classic image classification to medical image analysis. Designing and debugging custom CNNs, experimenting with pretrained backbones, and handling real-world dataset challenges sharpened both our engineering and research skills.

Key takeaways include:
\begin{itemize}
    \item Pretrained models require careful adaptation, especially in domain-shifted settings like medical imaging
    \item Multitask learning, although promising, introduces complexity in data flow, loss balancing, and resource usage
    \item Modular code and early visualizations greatly accelerate debugging and result interpretation
\end{itemize}

We believe our final models and analysis reflect a thoughtful balance of practical engineering, experimentation, and interpretability.

Future work could explore:
\begin{itemize}
    \item Attention-based mechanisms to improve focus on abnormal regions
    \item Transformer-based architectures for multimodal representation
    \item Additional modalities (e.g., metadata, clinical notes) if available
\end{itemize}

\end{document}
